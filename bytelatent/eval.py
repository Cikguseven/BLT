# Copyright (c) Meta Platforms, Inc. and affiliates.

import json
import logging
import math
import os
import sys
from datetime import datetime

os.environ["HF_ALLOW_CODE_EVAL"] = "1"
sys.path.append(os.path.join(os.path.dirname(__file__), ".."))

import torch
from lm_eval import simple_evaluate
from lm_eval.api.instance import Instance
from lm_eval.api.model import LM
from torch.nn import functional as F

from bytelatent.args import EvalArgs
from bytelatent.checkpoint import consolidate_checkpoints
from bytelatent.config_parser import parse_args_to_pydantic_model
from bytelatent.data.file_util import get_fs
from bytelatent.data.iterators.arrow_iterator import ArrowFileIterator
from bytelatent.data.iterators.limit_iterator import LimitIterator
from bytelatent.data.iterators.packing_iterator import (
    PackingArgs,
    PackingIterator,
    PackingMode,
)
from bytelatent.data.iterators.preprocess_iterator import PreprocessIterator
from bytelatent.data.iterators.sequence_iterator import (
    SequenceIterator,
    SequencePackingArgs,
)
from bytelatent.data.patcher import PatcherArgs, PatchingModeEnum
from bytelatent.distributed import (
    DistributedArgs,
    dist_mean_dict,
    dist_sum,
    get_device_mesh,
    get_global_rank,
    get_world_size,
    setup_torch_distributed,
    to_py_num,
)
from bytelatent.generate import (
    load_consolidated_model_and_tokenizer,
)
from bytelatent.generate_blt import generate_nocache
from bytelatent.model.blt import ByteLatentTransformer
from bytelatent.blt_tokenizers.build_tokenizer import TokenizerArgs
from bytelatent.transformer import LMTransformer

EVAL_FOLDER_NAME = "{:010d}"

logger = logging.getLogger()

def all_dicts_same(dict_list):
    if not dict_list:  # Check if the list is empty
        return True

    # Compare each dictionary to the first one
    first_dict = dict_list[0]
    return all(d == first_dict for d in dict_list)

class MockAccelerator:
    def gather(self, tensor):
        l = [torch.zeros_like(tensor) for _ in range(get_world_size())]
        torch.distributed.all_gather(l, tensor)
        return torch.stack(l)

    def wait_for_everyone(self):
        torch.distributed.barrier()

# Light wrapper around generator for lm-eval harness
class EvalHarnessLM(LM):
    def __init__(self, model, tokenizer, patcher):
        super().__init__()
        self.model = model
        self.tokenizer = tokenizer
        self.patcher = patcher
        self.accelerator = MockAccelerator()
        self._rank = get_global_rank()
        self._world_size = get_world_size()
        self.device = next(model.parameters()).device

        # Infer max sequence length
        if hasattr(model, "get_output_seq_len"):
            self.max_seq_len = model.get_output_seq_len()
        else:
            self.max_seq_len = 4096

    def generate_until(self, requests: list[Instance]) -> list[str]:
        results = [None] * len(requests)
        # Group by generation arguments
        groups = {}
        for i, req in enumerate(requests):
            prompt, gen_args = req.args
            # Convert dict to hashable tuple, handling list values like 'until'
            arg_key = tuple(sorted((k, tuple(v) if isinstance(v, list) else v)
                            for k, v in gen_args.items()))
            if arg_key not in groups:
                groups[arg_key] = []
            groups[arg_key].append((i, prompt, gen_args))

        for group in groups.values():
            indices, prompts, gen_args_list = zip(*group)
            gen_args = gen_args_list[0]

            temperature = gen_args.get("temperature", 0.0)
            use_sampling = temperature > 0.0
            temp_val = temperature if use_sampling else 1.0

            top_p = gen_args.get("top_p", 0.0)
            top_k = gen_args.get("top_k", 0)
            until = gen_args.get("until", [])

            # Get max_gen_len from generation_kwargs - use 'max_length' from gen_args
            max_gen_len = gen_args.get("max_length", 1024)

            # Be much more conservative with prompt length to avoid CUDA OOB errors.
            # BLT models have patching overhead, so we need a larger safety margin.
            # Use at most half the sequence length for the prompt, and cap gen_len too.
            max_gen_len = min(max_gen_len, self.max_seq_len // 2)
            max_prompt_len = max(1, self.max_seq_len - max_gen_len - 128)

            logger.info(f"Generating for {len(prompts)} prompts with max_gen_len={max_gen_len}, max_prompt_len={max_prompt_len}, max_seq_len={self.max_seq_len}")

            # Truncate prompts that are too long (by bytes) before passing to generate
            truncated_prompts = []
            for p in prompts:
                encoded = self.tokenizer.encode(p, add_bos=False, add_eos=False)
                if len(encoded) > max_prompt_len:
                    encoded = encoded[-max_prompt_len:]
                    p = self.tokenizer.decode(encoded)
                truncated_prompts.append(p)

            # Process one prompt at a time to isolate failures
            for idx, prompt_text in zip(indices, truncated_prompts):
                try:
                    torch.cuda.empty_cache()
                    generated_tokens_list = generate_nocache(
                        prompts=[prompt_text],
                        model=self.model,
                        tokenizer=self.tokenizer,
                        patcher=self.patcher,
                        max_prompt_len=max_prompt_len,
                        max_gen_len=max_gen_len,
                        use_sampling=use_sampling,
                        temp=temp_val,
                        top_k=top_k,
                        top_p=top_p,
                        remove_prompts=True
                    )

                    ids = generated_tokens_list[0]
                    text = self.tokenizer.decode(ids)

                    # Apply until stopping criteria
                    for stop_str in until:
                        if stop_str in text:
                            text = text.split(stop_str)[0]

                    results[idx] = text

                    # Log first few generations for debugging
                    if idx < 3:
                        logger.info(f"Generated for prompt {idx}: {text[:200]}...")

                except Exception as e:
                    logger.error(f"Error during generation for prompt {idx}: {e}", exc_info=True)
                    results[idx] = ""
                    # After a CUDA error, try to reset the state
                    try:
                        torch.cuda.synchronize()
                    except Exception:
                        pass
                    torch.cuda.empty_cache()

        return results


    def loglikelihood(self, requests: list[Instance]) -> list[tuple[float, bool]]:
        results = []
        for req in requests:
            prompt, continuation = req.args

            try:
                ctx_ids = self.tokenizer.encode(prompt, add_bos=True, add_eos=False)
                cont_ids = self.tokenizer.encode(continuation, add_bos=False, add_eos=False)

                # If there's no continuation, define loglikelihood=0 and greedy=True
                if len(cont_ids) == 0:
                    results.append((0.0, True))
                    continue

                # Use max_seq_len - 1 as absolute limit to prevent off-by-one errors
                max_total_len = self.max_seq_len - 1

                if len(cont_ids) >= max_total_len:
                    cont_ids = cont_ids[:max(1, max_total_len - 1)]

                ctx_budget = max_total_len - len(cont_ids)
                ctx_budget = max(1, ctx_budget)

                if len(ctx_ids) > ctx_budget:
                    ctx_ids = ctx_ids[-ctx_budget:]

                # (Defensive) if encode ever returned empty, skip rather than crash kernels
                if len(ctx_ids) == 0:
                    results.append((0.0, False))
                    continue

                full_ids = ctx_ids + cont_ids
                # Add extra check
                if len(full_ids) >= self.max_seq_len:
                    full_ids = full_ids[:self.max_seq_len - 1]

                tokens = torch.tensor([full_ids], device=self.device, dtype=torch.long)

                # Patching — use CPU tokens for patcher to avoid device conflicts
                tokens_cpu = tokens.cpu()
                patch_lengths, _ = self.patcher.patch(tokens_cpu, include_next_token=True)
                if patch_lengths is not None:
                    # Verify patch count doesn't exceed limit
                    if patch_lengths.size(1) >= self.max_seq_len:
                        logger.warning(f"Patch count {patch_lengths.size(1)} exceeds max {self.max_seq_len}, skipping")
                        results.append((0.0, False))
                        continue
                    patch_lengths = patch_lengths.to(self.device)

                with torch.no_grad():
                    logits = self.model(tokens, patch_lengths=patch_lengths)

                # Continuation is predicted starting from the last context token position.
                start_logit_idx = len(ctx_ids) - 1
                cont_len = len(cont_ids)
                end_logit_idx = start_logit_idx + cont_len

                # Clamp to what the model actually returned (extra safety)
                end_logit_idx = min(end_logit_idx, logits.size(1))
                cont_len = min(cont_len, end_logit_idx - start_logit_idx)

                if cont_len <= 0:
                    results.append((0.0, False))
                    continue

                relevant_logits = logits[0, start_logit_idx:end_logit_idx, :]
                target_tokens = tokens[0, len(ctx_ids) : len(ctx_ids) + cont_len]

                # Extra guard against any mismatch (prevents CUDA asserts)
                if relevant_logits.size(0) != target_tokens.numel():
                    n = min(relevant_logits.size(0), target_tokens.numel())
                    relevant_logits = relevant_logits[:n, :]
                    target_tokens = target_tokens[:n]

                loss = F.cross_entropy(relevant_logits, target_tokens, reduction="sum")

                greedy_tokens = relevant_logits.argmax(dim=-1)
                is_greedy = (greedy_tokens == target_tokens).all().item()

                results.append((-loss.item(), is_greedy))

            except Exception as e:
                logger.error(f"Error in loglikelihood: {e}", exc_info=True)
                results.append((0.0, False))

        return results

    def loglikelihood_rolling(self, requests: list[Instance]) -> list[float]:
        results = []
        for req in requests:
            try:
                prompt = req.args[0]
                tokens_list = self.tokenizer.encode(prompt, add_bos=True, add_eos=False)
                tokens = torch.tensor([tokens_list], device=self.device, dtype=torch.long)

                # Patching — use CPU tokens for patcher to avoid device conflicts
                tokens_cpu = tokens.cpu()
                patch_lengths, _ = self.patcher.patch(tokens_cpu, include_next_token=True)
                if patch_lengths is not None:
                    patch_lengths = patch_lengths.to(self.device)

                with torch.no_grad():
                    logits = self.model(tokens, patch_lengths=patch_lengths)

                shift_logits = logits[0, :-1, :].contiguous()
                shift_labels = tokens[0, 1:].contiguous()

                loss = F.cross_entropy(shift_logits, shift_labels, reduction="sum")
                results.append(-loss.item())

            except Exception as e:
                logger.error(f"Error in loglikelihood_rolling: {e}", exc_info=True)
                results.append(0.0)

        return results


@torch.no_grad()
def eval_ppl_on_path(
    *,
    world_rank: int,
    world_size: int,
    model: LMTransformer | ByteLatentTransformer,
    tokenizer_args: TokenizerArgs,
    patcher_args: PatcherArgs,
    packing_args: PackingArgs,
    add_patches: bool,
    path: str,
    arrow_batch_size: int,
    max_n_docs: int | None,
    max_n_batches: int | None,
    s3_profile: str | None = None,
):
    model.eval()
    seq_len = model.get_output_seq_len()
    arrow_iterator = ArrowFileIterator(
        file_path=None,
        dataset_files=[path],
        entropy_model_name=None,
        worker_id=world_rank,
        num_workers=world_size,
        arrow_batch_size=arrow_batch_size,
        preprocess_dir=None,
        s3_profile=s3_profile,
        file_format="arrow" if path.endswith("arrow") else "json",
    )
    if max_n_docs is not None:
        arrow_iterator = LimitIterator(arrow_iterator, limit=max_n_docs)
    preprocess_iterator = PreprocessIterator(
        arrow_iterator,
        patcher_args=patcher_args,
        tokenizer_args=tokenizer_args,
        add_patches=add_patches,
    )
    sequence_iterator = SequenceIterator(
        preprocess_iterator,
        sequence_packing_args=SequencePackingArgs(
            output_seq_len=seq_len,
            # Effectively disables shuffles
            buffer_size=1,
        ),
        rng_state=None,
    )
    packing_iterator = PackingIterator(sequence_iterator, packing_args=packing_args)
    total_loss = 0.0
    n_bytes = 0
    batch_iterator = packing_iterator.create_iter()
    for i, batch in enumerate(batch_iterator):
        if i == max_n_batches:
            break
        x = torch.from_numpy(batch.x).cuda()
        y = torch.from_numpy(batch.y).cuda()
        mask = None if batch.mask is None else torch.from_numpy(batch.mask).cuda()
        patch_lengths = batch.patch_lengths
        if patch_lengths is not None:
            patch_lengths = torch.from_numpy(patch_lengths).cuda()

        if tokenizer_args.name in ["bytes", "blt"]:
            n_bytes += y.numel() if mask is None else mask.sum().item()
            if isinstance(model, ByteLatentTransformer):
                pred = model(x, patch_lengths=patch_lengths)
            else:
                pred = model(x)
            loss = F.cross_entropy(
                pred.flatten(0, 1), y.flatten(0, 1), reduction="sum", ignore_index=0
            )
            total_loss += loss.item()
        else:
            raise NotImplementedError()
    all_n_bytes = to_py_num(dist_sum(n_bytes))
    all_total_loss = to_py_num(dist_sum(total_loss))
    return {
        "n_bytes": all_n_bytes,
        "n_bytes_gpu": n_bytes,
        "loss_sum": all_total_loss,
        "loss_sum_gpu": total_loss,
        "loss_mean": all_total_loss / all_n_bytes,
        "loss_mean_gpu": total_loss / n_bytes,
        "ppl": math.exp(all_total_loss / all_n_bytes) if all_n_bytes > 0 else 0.0,
        "bpb": all_total_loss / math.log(2) / all_n_bytes,
    }


def launch_eval(eval_args: EvalArgs):
    assert eval_args.ckpt_dir is not None

    timestamp = datetime.now().strftime("%b%d-%H%M")
    dump_dir = f"{eval_args.dump_dir}_{timestamp}"

    distributed_args = DistributedArgs()
    distributed_args.configure_world()
    if not torch.distributed.is_initialized():
        setup_torch_distributed(distributed_args)

    torch._dynamo.config.suppress_errors = True

    world_mesh = get_device_mesh(distributed_args)
    dp_mesh = world_mesh["dp_replicate"]
    assert distributed_args.dp_shard == 1
    world_size = dp_mesh.size()
    world_rank = dp_mesh.get_local_rank()

    fs = get_fs(eval_args.ckpt_dir, s3_profile=eval_args.s3_profile)
    if (
        fs.exists(eval_args.ckpt_dir)
        and fs.exists(os.path.join(eval_args.ckpt_dir, "params.json"))
        and len(fs.glob(os.path.join(eval_args.ckpt_dir, "*.pth"))) != 0
    ):
        consolidate_path = eval_args.ckpt_dir
    else:
        if eval_args.consolidate_if_needed:
            logger.info(
                "Found a model checkpoint, but it has not been consolidated.... so consolidating the checkpoint"
            )
            consolidate_path = os.path.join(
                eval_args.ckpt_dir, eval_args.consolidate_folder
            )
            if not fs.exists(consolidate_path) and get_global_rank() == 0:
                consolidate_path = consolidate_checkpoints(fs, eval_args.ckpt_dir)
            logger.info("Model consolidated to: %s", consolidate_path)
        else:
            raise ValueError(
                "Did not find a consolidated checkpoint and consolidate_if_needed is False"
            )

    fs.mkdirs(dump_dir, exist_ok=True)
    with fs.open(os.path.join(dump_dir, "config.yaml"), "w") as f:
        f.write(eval_args.model_dump_json())

    torch.distributed.barrier()
    logger.info("Loading model")
    model, tokenizer, train_cfg = load_consolidated_model_and_tokenizer(
        consolidate_path,
    )
    pad_id = 0 if train_cfg.data.tokenizer_args.name == "bytes" else tokenizer.boe_id
    model.eval()
    logger.info("Model loaded")

    ppl_results = None
    if eval_args.run_ppl:
        assert eval_args.validation is not None
        packing_args = PackingArgs(
            batch_size=eval_args.validation.batch_size,
            seq_len=train_cfg.data.seq_len,
            max_length=train_cfg.data.max_encoder_seq_length,
            pad_to_max_length=True,
            enable_byte_ngrams=False,
            pad_id=pad_id,
            packing_mode=(
                PackingMode.BYTES
                if train_cfg.data.patcher_args.patching_mode == PatchingModeEnum.byte
                else PackingMode.PATCHING
            ),
        )
        if len(eval_args.validation.sources) > 0:
            ppl_results = {}
            logger.info("Starting PPL evaluation on validation sets")
            for source in eval_args.validation.sources:
                ppl_results[source] = eval_ppl_on_path(
                    world_rank=world_rank,
                    world_size=world_size,
                    model=model,
                    tokenizer_args=train_cfg.data.tokenizer_args,
                    patcher_args=train_cfg.data.patcher_args,
                    packing_args=packing_args,
                    add_patches=train_cfg.data.add_patches,
                    path=os.path.join(eval_args.validation.root_dir, source),
                    max_n_docs=eval_args.validation.max_n_docs,
                    max_n_batches=eval_args.validation.max_n_batches,
                    arrow_batch_size=20,
                    s3_profile=eval_args.s3_profile,
                )

    # Build Patcher
    patcher_args = train_cfg.data.patcher_args.model_copy(deep=True)
    patcher_args.realtime_patching = True
    patcher_args.entropy_model_checkpoint_dir = eval_args.entropy_ckpt_dir
    patcher = patcher_args.build()

    task_results = None
    if eval_args.run_tasks:
        assert eval_args.harness is not None
        # Instantiate modified EvalHarnessLM
        wrap = EvalHarnessLM(model, tokenizer, patcher)
        logger.info("Starting evaluation with lm-eval harness")
        # Add confirm_run_unsafe_code=True here
        task_results = simple_evaluate(
            wrap,
            **eval_args.harness.model_dump(),
            confirm_run_unsafe_code=True
        )
    results = {"ppl": ppl_results, "tasks": task_results}
    # TODO: Serial and Parallel yield slightly different number of bytes, debug this later,
    # leaving this log statement here to help with that.
    # logging.info("Rank: %s Results: %s", world_rank, results)

    if get_global_rank() == 0:
        with fs.open(os.path.join(dump_dir, "results.json"), "w") as f:
            f.write(json.dumps(results, default=str))
        logger.info(f"All evaluation results: {results}")
        if ppl_results is not None:
            with fs.open(os.path.join(eval_args.dump_dir, "validation.json"), "w") as f:
                f.write(json.dumps(ppl_results))
            logger.info(f"All validation results: {ppl_results}")

    if eval_args.metric_log_dir and get_global_rank() == 0:
        metric_log_path = os.path.join(eval_args.metric_log_dir, "metrics.eval.jsonl")

        logger.info(f"Writing metric logs to {metric_log_path}")

        # Ensure directory exists
        fs.mkdirs(eval_args.metric_log_dir, exist_ok=True)

        timestamp: dict[str, int | str] = {
            "created_at": datetime.utcnow().isoformat(),
        }
        if eval_args.global_step is not None:
            timestamp["global_step"] = eval_args.global_step

        try:
            try:
                with fs.open(metric_log_path, "a") as f:
                    f.write(json.dumps(timestamp | results, default=str) + "\n")
                    f.flush()
            except (KeyError, TypeError):
                with fs.open(metric_log_path, "a") as f:
                    f.write(json.dumps(timestamp | results, default=str) + "\n")
                    f.flush()
        except Exception as e:
            logger.warning(f"Failed to write metrics to {metric_log_path}: {e}")
            fallback_path = os.path.join(dump_dir, "metrics.eval.jsonl")
            logger.info(f"Writing metrics to fallback location: {fallback_path}")
            with fs.open(fallback_path, "w") as f:
                f.write(json.dumps(timestamp | results, default=str) + "\n")


def main():
    eval_args = parse_args_to_pydantic_model(EvalArgs, cli_args="/scratch/Projects/CFP-01/CFP01-CF-060/kieron/blt/apps/main/configs/eval.yaml")
    launch_eval(eval_args)

if __name__ == "__main__":
    main()
